{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] =\"ETL-setup.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from local machine to bigquery - staging area\n",
    "def load_csv_to_bigquery(csv_path, project_id, table_name):\n",
    "    dataset_name = 'staging'\n",
    "    # Create a BigQuery client using your service account key file\n",
    "    #credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Read the CSV file into a Pandas dataframe\n",
    "    df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
    "\n",
    "    # Create the BigQuery dataset if it doesn't exist\n",
    "    dataset_ref = client.dataset(dataset_name)\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)\n",
    "        print(\"Dataset {} already exists\".format(dataset_name))\n",
    "    except:\n",
    "        print(\"Creating dataset {}\".format(dataset_name))\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        client.create_dataset(dataset)\n",
    "\n",
    "    # Set the destination table for the data\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "    # Define the schema of the table\n",
    "    schema = []\n",
    "    for column in df.columns:\n",
    "        schema.append(bigquery.SchemaField(column, 'STRING'))\n",
    "\n",
    "    # Create the table in BigQuery\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "    table = client.create_table(table)\n",
    "\n",
    "    # Load the data into the table\n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.CSV\n",
    "    job_config.skip_leading_rows = 1\n",
    "    job_config.autodetect = False # Set to True to automatically detect schema, False to use schema defined above\n",
    "    job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(\"Data uploaded to BigQuery successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_bigquery_table(project_id, table_id, remove_nulls=False, remove_duplicates=False, date_columns=None, columns_to_check=None):\n",
    "    \"\"\"\n",
    "    Clean a BigQuery table by removing null values and/or duplicates.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): The Google Cloud Project ID.\n",
    "        table_id (str): The BigQuery table ID.\n",
    "        remove_nulls (bool, optional): Whether to remove rows with null values. Defaults to False.\n",
    "        columns_to_check (list, optional): List of columns to check for null values or duplicates. Defaults to None (all columns).\n",
    "        remove_duplicates (bool, optional): Whether to remove duplicate rows. Defaults to False.\n",
    "        date_columns (list, optional): List of columns to convert to date format. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    table_ref = client.get_table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    if columns_to_check is None:\n",
    "        columns_to_check = [field.name for field in table.schema]\n",
    "\n",
    "    sql_base = f\"SELECT * FROM `{table_id}`\"\n",
    "    sql_conditions = []\n",
    "\n",
    "    if remove_nulls:\n",
    "        not_null_conditions = [f\"{column} IS NOT NULL\" for column in columns_to_check]\n",
    "        sql_conditions.append(\" AND \".join(not_null_conditions))\n",
    "\n",
    "    if remove_duplicates:\n",
    "        deduplicate_clause = \"SELECT DISTINCT\"\n",
    "    else:\n",
    "        deduplicate_clause = \"SELECT\"\n",
    "\n",
    "    if sql_conditions:\n",
    "        sql_condition = \"WHERE \" + \" AND \".join(sql_conditions)\n",
    "    else:\n",
    "        sql_condition = \"\"\n",
    "\n",
    "    # Handle date column transformation, and make all columns lower case\n",
    "    select_columns = []\n",
    "    for column in table.schema:\n",
    "        if column.name in date_columns:\n",
    "            select_columns.append(f\"PARSE_DATE('%d-%m-%Y', REGEXP_REPLACE({column.name}, r'/', '-')) AS {column.name.lower()}\")\n",
    "        else:\n",
    "            select_columns.append(column.name.lower())\n",
    "\n",
    "\n",
    "        sql = f\"{deduplicate_clause} {', '.join(select_columns)} FROM ({sql_base}) AS subquery {sql_condition}\"\n",
    "\n",
    "    # Execute the query and save the results to a new table\n",
    "    new_table_id = f\"{project_id}.{table_ref.dataset_id}.{table_ref.table_id}_cleaned\"\n",
    "    new_table_ref = client.dataset(table_ref.dataset_id).table(f\"{table_ref.table_id}_cleaned\")\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig(destination=new_table_ref)\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "    query_job.result()\n",
    "\n",
    "    print(f\"Cleaned table saved as {new_table_id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset staging already exists\n"
     ]
    },
    {
     "ename": "Conflict",
     "evalue": "409 POST https://bigquery.googleapis.com/bigquery/v2/projects/snappy-nomad-382716/datasets/staging/tables?prettyPrint=false: Already Exists: Table snappy-nomad-382716:staging.superstore",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m load_csv_to_bigquery(\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUX501VW\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mBigQuery-Data-Modeling\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDastaset\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39msuperstore_dataset2011-2015.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msnappy-nomad-382716\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msuperstore\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[26], line 31\u001b[0m, in \u001b[0;36mload_csv_to_bigquery\u001b[1;34m(csv_path, project_id, table_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39m# Create the table in BigQuery\u001b[39;00m\n\u001b[0;32m     30\u001b[0m table \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mTable(table_ref, schema\u001b[39m=\u001b[39mschema)\n\u001b[1;32m---> 31\u001b[0m table \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate_table(table)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Load the data into the table\u001b[39;00m\n\u001b[0;32m     34\u001b[0m job_config \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mLoadJobConfig()\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:779\u001b[0m, in \u001b[0;36mClient.create_table\u001b[1;34m(self, table, exists_ok, retry, timeout)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m     span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path, \u001b[39m\"\u001b[39m\u001b[39mdataset_id\u001b[39m\u001b[39m\"\u001b[39m: dataset_id}\n\u001b[1;32m--> 779\u001b[0m     api_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(\n\u001b[0;32m    780\u001b[0m         retry,\n\u001b[0;32m    781\u001b[0m         span_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBigQuery.createTable\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    782\u001b[0m         span_attributes\u001b[39m=\u001b[39;49mspan_attributes,\n\u001b[0;32m    783\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    784\u001b[0m         path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    785\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[0;32m    786\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    787\u001b[0m     )\n\u001b[0;32m    788\u001b[0m     \u001b[39mreturn\u001b[39;00m Table\u001b[39m.\u001b[39mfrom_api_repr(api_response)\n\u001b[0;32m    789\u001b[0m \u001b[39mexcept\u001b[39;00m core_exceptions\u001b[39m.\u001b[39mConflict:\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:813\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[39mif\u001b[39;00m span_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[0;32m    811\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[0;32m    812\u001b[0m     ):\n\u001b[1;32m--> 813\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    350\u001b[0m     target,\n\u001b[0;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    352\u001b[0m     sleep_generator,\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[0;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    193\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    482\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[39m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m expect_json \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mcontent:\n\u001b[0;32m    497\u001b[0m     \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mjson()\n",
      "\u001b[1;31mConflict\u001b[0m: 409 POST https://bigquery.googleapis.com/bigquery/v2/projects/snappy-nomad-382716/datasets/staging/tables?prettyPrint=false: Already Exists: Table snappy-nomad-382716:staging.superstore"
     ]
    }
   ],
   "source": [
    "load_csv_to_bigquery(\"C:\\\\Users\\\\UX501VW\\\\Desktop\\\\BigQuery-Data-Modeling\\\\Dastaset\\\\superstore_dataset2011-2015.csv\", \"snappy-nomad-382716\", \"superstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned table saved as snappy-nomad-382716.staging.superstore_cleaned.\n"
     ]
    }
   ],
   "source": [
    "clean_bigquery_table(\"snappy-nomad-382716\", \"snappy-nomad-382716.staging.superstore\", remove_nulls=True, remove_duplicates=True, date_columns=[\"Order_Date\", \"Ship_Date\"], columns_to_check=[\"Customer_ID\", \"Order_Date\", \"Order_ID\", \"Product_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_table_name = \"sales_fact\"\n",
    "fact_table_columns = [\n",
    "    bigquery.SchemaField(\"order_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"product_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"customer_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"sales\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"profit\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "]\n",
    "\n",
    "dimension_tables = {\n",
    "    \"date_dim\": [\n",
    "        bigquery.SchemaField(\"date\", \"DATE\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"quarter\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "    ],\n",
    "    \"customer_dim\": [\n",
    "        bigquery.SchemaField(\"customer_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"customer_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"segment\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"country\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"city\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"state\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"postal_code\", \"INTEGER\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"region\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    ],\n",
    "    \"product_dim\": [\n",
    "        bigquery.SchemaField(\"product_id\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"category\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"sub_category\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"product_name\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "fact_dimension_key_map = {\n",
    "    \"order_id\": {\n",
    "        \"date_dim_date\": \"date_dim\",\n",
    "        \"customer_dim_customer_id\": \"customer_dim\",\n",
    "        \"product_dim_product_id\": \"product_dim\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def create_star_schema_from_columns(\n",
    "    project_id,\n",
    "    staging_dataset_id,\n",
    "    source_table_name,\n",
    "    fact_table_name,\n",
    "    fact_table_columns,\n",
    "    dimension_table_columns_map\n",
    "):\n",
    "    # Create a client\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    staging_dataset_ref = client.dataset(staging_dataset_id)\n",
    "\n",
    "    # Create the warehouse dataset\n",
    "    warehouse_dataset_id = f\"{staging_dataset_id}_warehouse\"\n",
    "    warehouse_dataset_ref = client.dataset(warehouse_dataset_id)\n",
    "    try:\n",
    "        client.get_dataset(warehouse_dataset_ref)\n",
    "        print(f\"Warehouse dataset {warehouse_dataset_id} already exists.\")\n",
    "    except NotFound:\n",
    "        warehouse_dataset = bigquery.Dataset(warehouse_dataset_ref)\n",
    "        client.create_dataset(warehouse_dataset)\n",
    "        print(f\"Warehouse dataset {warehouse_dataset_id} created.\")\n",
    "    \n",
    "    # Get the source table\n",
    "    source_table_ref = staging_dataset_ref.table(source_table_name)\n",
    "    source_table = client.get_table(source_table_ref)\n",
    "\n",
    "    # Create fact table\n",
    "    fact_table_ref = warehouse_dataset_ref.table(fact_table_name)\n",
    "    fact_table = bigquery.Table(fact_table_ref, schema=fact_table_columns)\n",
    "    client.create_table(fact_table)\n",
    "    print(f\"Fact table {fact_table_name} created.\")\n",
    "\n",
    "    # Create dimension tables\n",
    "    for dimension_table_name, dimension_columns in dimension_table_columns_map.items():\n",
    "        dimension_table_ref = warehouse_dataset_ref.table(dimension_table_name)\n",
    "        dimension_table = bigquery.Table(dimension_table_ref, schema=dimension_columns)\n",
    "        client.create_table(dimension_table)\n",
    "        print(f\"Dimension table {dimension_table_name} created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NotFound' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m, in \u001b[0;36mcreate_star_schema_from_columns\u001b[1;34m(project_id, staging_dataset_id, source_table_name, fact_table_name, fact_table_columns, dimension_table_columns_map)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     client\u001b[39m.\u001b[39;49mget_dataset(warehouse_dataset_ref)\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWarehouse dataset \u001b[39m\u001b[39m{\u001b[39;00mwarehouse_dataset_id\u001b[39m}\u001b[39;00m\u001b[39m already exists.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:850\u001b[0m, in \u001b[0;36mClient.get_dataset\u001b[1;34m(self, dataset_ref, retry, timeout)\u001b[0m\n\u001b[0;32m    849\u001b[0m span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path}\n\u001b[1;32m--> 850\u001b[0m api_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_api(\n\u001b[0;32m    851\u001b[0m     retry,\n\u001b[0;32m    852\u001b[0m     span_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBigQuery.getDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    853\u001b[0m     span_attributes\u001b[39m=\u001b[39;49mspan_attributes,\n\u001b[0;32m    854\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    855\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m    856\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    858\u001b[0m \u001b[39mreturn\u001b[39;00m Dataset\u001b[39m.\u001b[39mfrom_api_repr(api_response)\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\bigquery\\client.py:813\u001b[0m, in \u001b[0;36mClient._call_api\u001b[1;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[0;32m    811\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[0;32m    812\u001b[0m     ):\n\u001b[1;32m--> 813\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[0;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    350\u001b[0m     target,\n\u001b[0;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[0;32m    352\u001b[0m     sleep_generator,\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[0;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mreturn\u001b[39;00m target()\n\u001b[0;32m    193\u001b[0m \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\UX501VW\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[39mif\u001b[39;00m expect_json \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mcontent:\n",
      "\u001b[1;31mNotFound\u001b[0m: 404 GET https://bigquery.googleapis.com/bigquery/v2/projects/snappy-nomad-382716/datasets/snappy-nomad-382716.staging_warehouse?prettyPrint=false: <!DOCTYPE html>\n<html lang=en>\n  <meta charset=utf-8>\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n  <title>Error 404 (Not Found)!!1</title>\n  <style>\n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  </style>\n  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n  <p><b>404.</b> <ins>That’s an error.</ins>\n  <p>The requested URL <code>/bigquery/v2/projects/snappy-nomad-382716/datasets/snappy-nomad-382716.staging_warehouse?prettyPrint=false</code> was not found on this server.  <ins>That’s all we know.</ins>\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_star_schema_from_columns(\n\u001b[0;32m      2\u001b[0m     project_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msnappy-nomad-382716\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m     staging_dataset_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msnappy-nomad-382716.staging\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     source_table_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msuperstore_cleaned\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      5\u001b[0m     fact_table_name\u001b[39m=\u001b[39;49mfact_table_name,\n\u001b[0;32m      6\u001b[0m     fact_table_columns\u001b[39m=\u001b[39;49mfact_table_columns,\n\u001b[0;32m      7\u001b[0m     dimension_table_columns_map\u001b[39m=\u001b[39;49mdimension_tables,\n\u001b[0;32m      8\u001b[0m )\n",
      "Cell \u001b[1;32mIn[36], line 21\u001b[0m, in \u001b[0;36mcreate_star_schema_from_columns\u001b[1;34m(project_id, staging_dataset_id, source_table_name, fact_table_name, fact_table_columns, dimension_table_columns_map)\u001b[0m\n\u001b[0;32m     19\u001b[0m     client\u001b[39m.\u001b[39mget_dataset(warehouse_dataset_ref)\n\u001b[0;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWarehouse dataset \u001b[39m\u001b[39m{\u001b[39;00mwarehouse_dataset_id\u001b[39m}\u001b[39;00m\u001b[39m already exists.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mexcept\u001b[39;00m NotFound:\n\u001b[0;32m     22\u001b[0m     warehouse_dataset \u001b[39m=\u001b[39m bigquery\u001b[39m.\u001b[39mDataset(warehouse_dataset_ref)\n\u001b[0;32m     23\u001b[0m     client\u001b[39m.\u001b[39mcreate_dataset(warehouse_dataset)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NotFound' is not defined"
     ]
    }
   ],
   "source": [
    "create_star_schema_from_columns(\n",
    "    project_id=\"snappy-nomad-382716\",\n",
    "    staging_dataset_id=\"snappy-nomad-382716.staging\",\n",
    "    source_table_name=\"superstore_cleaned\",\n",
    "    fact_table_name=fact_table_name,\n",
    "    fact_table_columns=fact_table_columns,\n",
    "    dimension_table_columns_map=dimension_tables,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_star_schema(\n",
    "#     project_id,\n",
    "#     staging_table_id,\n",
    "#     facts_table_name,\n",
    "#     facts_columns,\n",
    "#     dimensions,\n",
    "#     warehouse_dataset_name=\"warehouse\"\n",
    "# ):\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "\n",
    "#     # # Create the warehouse dataset if it doesn't exist\n",
    "#     # warehouse_dataset_id = f\"{project_id}.{warehouse_dataset_name}\"\n",
    "#     # try:\n",
    "#     #     client.get_dataset(warehouse_dataset_id)\n",
    "#     # except NotFound:\n",
    "#     #     warehouse_dataset = bigquery.Dataset(warehouse_dataset_id)\n",
    "#     #     client.create_dataset(warehouse_dataset)\n",
    "#     #     print(f\"Created dataset '{warehouse_dataset_name}'.\")\n",
    "        \n",
    "        \n",
    "#     dataset_ref = client.dataset(warehouse_dataset_name)\n",
    "#     try:\n",
    "#         client.get_dataset(dataset_ref)\n",
    "#         print(\"Dataset {} already exists\".format(warehouse_dataset_name))\n",
    "#     except:\n",
    "#         print(\"Creating dataset {}\".format(warehouse_dataset_name))\n",
    "#         dataset = bigquery.Dataset(dataset_ref)\n",
    "#         client.create_dataset(dataset)\n",
    "\n",
    "#     # Create the facts table\n",
    "#     facts_table_id = f\"{warehouse_dataset_name}.{facts_table_name}\"\n",
    "#     facts_schema = [bigquery.SchemaField(col_name, col_type) for col_name, col_type in facts_columns]\n",
    "#     facts_table = bigquery.Table(facts_table_id, schema=facts_schema)\n",
    "#     client.create_table(facts_table)\n",
    "#     print(f\"Created table '{facts_table_name}'.\")\n",
    "\n",
    "#     # Create the dimensions tables\n",
    "#     for dimension in dimensions:\n",
    "#         dim_table_name, dim_columns = dimension\n",
    "#         dim_table_id = f\"{warehouse_dataset_name}.{dim_table_name}\"\n",
    "#         dim_schema = [bigquery.SchemaField(col_name, col_type) for col_name, col_type in dim_columns]\n",
    "#         dim_table = bigquery.Table(dim_table_id, schema=dim_schema)\n",
    "#         client.create_table(dim_table)\n",
    "#         print(f\"Created table '{dim_table_name}'.\")\n",
    "        \n",
    "        \n",
    "#from google.cloud import bigquery\n",
    "# from google.api_core.exceptions import NotFound\n",
    "\n",
    "# def create_star_schema(\n",
    "#     project_id,\n",
    "#     staging_table_id,\n",
    "#     facts_table_name,\n",
    "#     facts_columns,\n",
    "#     dimensions,\n",
    "#     warehouse_dataset_name=\"warehouse\"\n",
    "# ):\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "\n",
    "#     # Create the warehouse dataset if it doesn't exist\n",
    "#         dataset_ref = client.dataset(warehouse_dataset_name)\n",
    "#     try:\n",
    "#         client.get_dataset(dataset_ref)\n",
    "#         print(\"Dataset {} already exists\".format(warehouse_dataset_name))\n",
    "#     except:\n",
    "#         print(\"Creating dataset {}\".format(warehouse_dataset_name))\n",
    "#         dataset = bigquery.Dataset(dataset_ref)\n",
    "#         client.create_dataset(dataset)\n",
    "\n",
    "#     # Create the facts table\n",
    "#     facts_table_id = f\"{project_id}.{warehouse_dataset_name}.{facts_table_name}\"\n",
    "#     facts_schema = [bigquery.SchemaField(col_name, col_type) for col_name, col_type in facts_columns]\n",
    "#     facts_table = bigquery.Table(facts_table_id, schema=facts_schema)\n",
    "#     client.create_table(facts_table)\n",
    "#     print(f\"Created table '{facts_table_name}'.\")\n",
    "\n",
    "#     # Create the dimensions tables\n",
    "#     for dimension in dimensions:\n",
    "#         dim_table_name, dim_columns = dimension\n",
    "#         dim_table_id = f\"{project_id}.{warehouse_dataset_name}.{dim_table_name}\"\n",
    "#         dim_schema = [bigquery.SchemaField(col_name, col_type) for col_name, col_type in dim_columns]\n",
    "#         dim_table = bigquery.Table(dim_table_id, schema=dim_schema)\n",
    "#         client.create_table(dim_table)\n",
    "#         print(f\"Created table '{dim_table_name}'.\")\n",
    "\n",
    "#from google.cloud import bigquery\n",
    "# from google.api_core.exceptions import NotFound\n",
    "\n",
    "# def create_star_schema(\n",
    "#     project_id,\n",
    "#     staging_table_id,\n",
    "#     facts_table_name,\n",
    "#     facts_columns,\n",
    "#     dimensions,\n",
    "#     warehouse_dataset_name=\"warehouse\"\n",
    "# ):\n",
    "#     client = bigquery.Client(project=project_id)\n",
    "\n",
    "#     # Create the warehouse dataset if it doesn't exist\n",
    "#     warehouse_dataset_id = f\"{project_id}.{warehouse_dataset_name}\"\n",
    "#     try:\n",
    "#         client.get_dataset(warehouse_dataset_id)\n",
    "#     except NotFound:\n",
    "#         warehouse_dataset = bigquery.Dataset(warehouse_dataset_id)\n",
    "#         client.create_dataset(warehouse_dataset)\n",
    "#         print(f\"Created dataset '{warehouse_dataset_name}'.\")\n",
    "\n",
    "#     # Create the facts table by selecting the required columns from the staging table\n",
    "#     facts_table_id = f\"{project_id}.{warehouse_dataset_name}.{facts_table_name}\"\n",
    "#     facts_select_columns = ', '.join([f\"staging.{col_name}\" for col_name, _ in facts_columns])\n",
    "\n",
    "#     facts_query = f\"\"\"\n",
    "#         CREATE TABLE `{facts_table_id}`\n",
    "#         AS SELECT {facts_select_columns}\n",
    "#         FROM `{staging_table_id}` staging\n",
    "#     \"\"\"\n",
    "#     client.query(facts_query).result()\n",
    "#     print(f\"Created table '{facts_table_name}'.\")\n",
    "\n",
    "#     # Create the dimensions tables\n",
    "#     for dimension in dimensions:\n",
    "#         dim_table_name, dim_columns = dimension\n",
    "#         dim_table_id = f\"{project_id}.{warehouse_dataset_name}.{dim_table_name}\"\n",
    "#         dim_select_columns = ', '.join([f\"staging.{col_name}\" for col_name, _ in dim_columns])\n",
    "\n",
    "#         dim_query = f\"\"\"\n",
    "#             CREATE TABLE `{dim_table_id}`\n",
    "#             AS SELECT DISTINCT {dim_select_columns}\n",
    "#             FROM `{staging_table_id}` staging\n",
    "#         \"\"\"\n",
    "#         client.query(dim_query).result()\n",
    "#         print(f\"Created table '{dim_table_name}'.\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import NotFound\n",
    "\n",
    "def create_star_schema(\n",
    "    project_id,\n",
    "    staging_table_id,\n",
    "    facts_table_name,\n",
    "    facts_columns,\n",
    "    dimensions,\n",
    "    warehouse_dataset_name=\"warehouse\"\n",
    "):\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    # Create the warehouse dataset if it doesn't exist\n",
    "    warehouse_dataset_id = f\"{project_id}.{warehouse_dataset_name}\"\n",
    "    try:\n",
    "        client.get_dataset(warehouse_dataset_id)\n",
    "    except NotFound:\n",
    "        warehouse_dataset = bigquery.Dataset(warehouse_dataset_id)\n",
    "        client.create_dataset(warehouse_dataset)\n",
    "        print(f\"Created dataset '{warehouse_dataset_name}'.\")\n",
    "\n",
    "    # Create the facts table by selecting the required columns from the staging table\n",
    "    facts_table_id = f\"{project_id}.{warehouse_dataset_name}.{facts_table_name}\"\n",
    "    facts_select_columns = ', '.join([f\"staging.{col_name}\" for col_name, _ in facts_columns])\n",
    "\n",
    "    facts_query = f\"\"\"\n",
    "        CREATE TABLE `{facts_table_id}`\n",
    "        AS SELECT {facts_select_columns}\n",
    "        FROM `{staging_table_id}` staging\n",
    "    \"\"\"\n",
    "    client.query(facts_query).result()\n",
    "    print(f\"Created table '{facts_table_name}'.\")\n",
    "\n",
    "    # Create the dimensions tables\n",
    "    for dimension in dimensions:\n",
    "        dim_table_name, dim_columns = dimension\n",
    "        dim_table_id = f\"{project_id}.{warehouse_dataset_name}.{dim_table_name}\"\n",
    "        dim_select_columns = ', '.join([f\"staging.{col_name}\" for col_name, _ in dim_columns])\n",
    "\n",
    "        dim_query = f\"\"\"\n",
    "            CREATE TABLE `{dim_table_id}`\n",
    "            AS SELECT DISTINCT {dim_select_columns}\n",
    "            FROM `{staging_table_id}` staging\n",
    "        \"\"\"\n",
    "        client.query(dim_query).result()\n",
    "        print(f\"Created table '{dim_table_name}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_superstore_star_schema(project_id, staging_table_id):\n",
    "    facts_table_name = \"order_facts\"\n",
    "    facts_columns = [\n",
    "        (\"order_id\", \"STRING\"),\n",
    "        (\"order_date\", \"DATE\"),\n",
    "        (\"ship_date\", \"DATE\"),\n",
    "        (\"sales\", \"FLOAT64\"),\n",
    "        (\"quantity\", \"INT64\"),\n",
    "        (\"discount\", \"FLOAT64\"),\n",
    "        (\"profit\", \"FLOAT64\"),\n",
    "        (\"customer_id\", \"STRING\"),\n",
    "        (\"product_id\", \"STRING\"),\n",
    "    ]\n",
    "\n",
    "    \n",
    "    dimensions = [\n",
    "        (\n",
    "            \"customers\",\n",
    "            [\n",
    "                (\"customer_id\", \"STRING\"),\n",
    "                (\"customer_name\", \"STRING\"),\n",
    "                (\"segment\", \"STRING\"),\n",
    "                #(\"region\", \"STRING\")\n",
    "                (\"country\", \"STRING\"),\n",
    "                (\"state\", \"STRING\"),\n",
    "                (\"city\", \"STRING\"),\n",
    "                (\"postal_code\", \"STRING\")\n",
    "                \n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"products\",\n",
    "            [\n",
    "                (\"product_id\", \"STRING\"),\n",
    "                (\"product_name\", \"STRING\"),\n",
    "                (\"category\", \"STRING\"),\n",
    "                (\"sub_category\", \"STRING\")\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"date_dim\",\n",
    "            [\n",
    "                (\"order_date\", \"DATE\"),\n",
    "                (\"ship_date\", \"DATE\")\n",
    "                # (\"day\", \"INTEGER\"),\n",
    "                # (\"month\", \"INTEGER\"),\n",
    "                # (\"year\", \"INTEGER\"),\n",
    "            ],\n",
    "        ),\n",
    "        \n",
    "    ]\n",
    "\n",
    "    create_star_schema(\n",
    "        project_id=project_id,\n",
    "        staging_table_id=staging_table_id,\n",
    "        facts_table_name=facts_table_name,\n",
    "        facts_columns=facts_columns,\n",
    "        dimensions=dimensions,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'order_facts'.\n",
      "Created table 'customers'.\n",
      "Created table 'products'.\n",
      "Created table 'date_dim'.\n"
     ]
    }
   ],
   "source": [
    "create_superstore_star_schema(\"snappy-nomad-382716\", \"snappy-nomad-382716.staging.superstore_cleaned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
